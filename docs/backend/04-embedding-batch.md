# Search V2 ì„ë² ë”© ë°°ì¹˜ ì²˜ë¦¬

Search V2ì˜ ë²¡í„° ì„ë² ë”© ìƒì„± ë° ê´€ë¦¬ë¥¼ ìœ„í•œ ë°°ì¹˜ ì²˜ë¦¬ ì‹œìŠ¤í…œ ì„¤ê³„ì…ë‹ˆë‹¤.

## ì„ë² ë”© ë°°ì¹˜ ì²˜ë¦¬ ê°œìš”

### ì²˜ë¦¬ íë¦„
1. **ì‹ ê·œ/ì—…ë°ì´íŠ¸ ê°ì§€**: embeddingì´ ì—†ê±°ë‚˜ outdatedëœ ì•„ì´í…œ ì‹ë³„
2. **í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬**: ì œëª©, ìš”ì•½, íƒœê·¸ë¥¼ ì„ë² ë”©ìš© í…ìŠ¤íŠ¸ë¡œ ê²°í•©
3. **ë°°ì¹˜ ìƒì„±**: OpenAI API í˜¸ì¶œ ìµœì í™”ë¥¼ ìœ„í•œ ë°°ì¹˜ êµ¬ì„±
4. **ì„ë² ë”© ìƒì„±**: OpenAI text-embedding-ada-002 API í˜¸ì¶œ
5. **DB ì—…ë°ì´íŠ¸**: ìƒì„±ëœ ì„ë² ë”©ì„ PostgreSQLì— ì €ì¥
6. **ì‹¤íŒ¨ ì²˜ë¦¬**: ì¬ì‹œë„ ë¡œì§ ë° ì˜¤ë¥˜ ì²˜ë¦¬

### ì„±ëŠ¥ ëª©í‘œ
- **ë°°ì¹˜ í¬ê¸°**: 100ê°œ ì•„ì´í…œ/ë°°ì¹˜ (OpenAI API ì œí•œ ê³ ë ¤)
- **ì²˜ë¦¬ ì†ë„**: 1000ê°œ ì•„ì´í…œ/ì‹œê°„
- **ì˜¤ë¥˜ìœ¨**: <1% (ì¬ì‹œë„ í¬í•¨)
- **ë¹„ìš© ìµœì í™”**: ì¤‘ë³µ ì„ë² ë”© ìƒì„± ë°©ì§€

## ë°°ì¹˜ ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸

### Node.js ë°°ì¹˜ ìŠ¤í¬ë¦½íŠ¸

```typescript
// scripts/embedding-batch.ts
import { Pool } from 'pg'
import { OpenAI } from 'openai'
import { Redis } from 'ioredis'

// =====================================================
// ì„¤ì • ë° ì´ˆê¸°í™”
// =====================================================

interface EmbeddingConfig {
  batchSize: number
  maxRetries: number
  delayBetweenBatches: number
  model: string
  maxTokens: number
}

const config: EmbeddingConfig = {
  batchSize: 100,
  maxRetries: 3,
  delayBetweenBatches: 2000, // 2ì´ˆ
  model: 'text-embedding-ada-002',
  maxTokens: 8000 // OpenAI ì œí•œ
}

const pool = new Pool({
  host: process.env.DB_HOST,
  port: parseInt(process.env.DB_PORT || '5432'),
  database: process.env.DB_NAME,
  user: process.env.DB_USER,
  password: process.env.DB_PASSWORD,
  max: 5 // ë°°ì¹˜ ì²˜ë¦¬ìš© ì ì€ ì—°ê²° ìˆ˜
})

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY
})

const redis = new Redis({
  host: process.env.REDIS_HOST,
  port: parseInt(process.env.REDIS_PORT || '6379'),
  password: process.env.REDIS_PASSWORD
})

// =====================================================
// íƒ€ì… ì •ì˜
// =====================================================

interface ItemForEmbedding {
  id: string
  title: string
  summary: string
  tags: string[]
  content_text: string
  token_count?: number
}

interface BatchResult {
  processed: number
  succeeded: number
  failed: number
  skipped: number
  errors: Array<{
    id: string
    error: string
  }>
}

interface EmbeddingJob {
  id: string
  status: 'pending' | 'processing' | 'completed' | 'failed'
  created_at: Date
  updated_at: Date
  batch_size: number
  progress: {
    total: number
    processed: number
    succeeded: number
    failed: number
  }
}

// =====================================================
// ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
// =====================================================

function prepareTextForEmbedding(item: ItemForEmbedding): string {
  const parts = [
    item.title,
    item.summary || '',
    Array.isArray(item.tags) ? item.tags.join(' ') : ''
  ].filter(Boolean)
  
  return parts.join(' ').trim()
}

function estimateTokenCount(text: string): number {
  // ê°„ë‹¨í•œ í† í° ì¶”ì • (ì •í™•í•˜ì§€ ì•Šì§€ë§Œ ë°°ì¹˜ ê³„íšìš©)
  return Math.ceil(text.length / 4)
}

function splitIntoBatches<T>(items: T[], batchSize: number): T[][] {
  const batches: T[][] = []
  for (let i = 0; i < items.length; i += batchSize) {
    batches.push(items.slice(i, i + batchSize))
  }
  return batches
}

async function delay(ms: number): Promise<void> {
  return new Promise(resolve => setTimeout(resolve, ms))
}

// =====================================================
// ë©”ì¸ ë°°ì¹˜ ì²˜ë¦¬ í´ë˜ìŠ¤
// =====================================================

class EmbeddingBatchProcessor {
  private readonly pool: Pool
  private readonly openai: OpenAI
  private readonly redis: Redis
  private readonly config: EmbeddingConfig

  constructor(pool: Pool, openai: OpenAI, redis: Redis, config: EmbeddingConfig) {
    this.pool = pool
    this.openai = openai
    this.redis = redis
    this.config = config
  }

  async processAllPendingItems(): Promise<BatchResult> {
    console.log('ğŸš€ ì„ë² ë”© ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘')
    
    // 1. ì²˜ë¦¬ ëŒ€ìƒ ì•„ì´í…œ ì¡°íšŒ
    const pendingItems = await this.getPendingItems()
    console.log(`ğŸ“Š ì²˜ë¦¬ ëŒ€ìƒ: ${pendingItems.length}ê°œ ì•„ì´í…œ`)
    
    if (pendingItems.length === 0) {
      console.log('âœ… ì²˜ë¦¬í•  ì•„ì´í…œ ì—†ìŒ')
      return { processed: 0, succeeded: 0, failed: 0, skipped: 0, errors: [] }
    }

    // 2. ë°°ì¹˜ë¡œ ë¶„í• 
    const batches = splitIntoBatches(pendingItems, this.config.batchSize)
    console.log(`ğŸ“¦ ${batches.length}ê°œ ë°°ì¹˜ë¡œ ë¶„í• `)

    // 3. ê° ë°°ì¹˜ ì²˜ë¦¬
    let totalResult: BatchResult = {
      processed: 0,
      succeeded: 0,
      failed: 0,
      skipped: 0,
      errors: []
    }

    for (let i = 0; i < batches.length; i++) {
      console.log(`ğŸ”„ ë°°ì¹˜ ${i + 1}/${batches.length} ì²˜ë¦¬ ì¤‘...`)
      
      const batchResult = await this.processBatch(batches[i])
      
      totalResult.processed += batchResult.processed
      totalResult.succeeded += batchResult.succeeded
      totalResult.failed += batchResult.failed
      totalResult.skipped += batchResult.skipped
      totalResult.errors.push(...batchResult.errors)
      
      // ë°°ì¹˜ ê°„ ëŒ€ê¸° (API ì œí•œ ì¤€ìˆ˜)
      if (i < batches.length - 1) {
        await delay(this.config.delayBetweenBatches)
      }
    }

    console.log('âœ… ì„ë² ë”© ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ')
    console.log(`ğŸ“ˆ ê²°ê³¼: ${totalResult.succeeded}ê°œ ì„±ê³µ, ${totalResult.failed}ê°œ ì‹¤íŒ¨`)
    
    return totalResult
  }

  private async getPendingItems(): Promise<ItemForEmbedding[]> {
    const query = `
      SELECT 
        id,
        title,
        summary,
        tags,
        CONCAT_WS(' ', title, COALESCE(summary, ''), array_to_string(COALESCE(tags, '{}'), ' ')) as content_text
      FROM ai_items
      WHERE 
        embedding IS NULL 
        OR embedding_updated_at IS NULL 
        OR embedding_updated_at < updated_at
      ORDER BY 
        CASE WHEN embedding IS NULL THEN 0 ELSE 1 END,  -- NULL ìš°ì„ 
        updated_at DESC
      LIMIT 5000  -- í•œ ë²ˆì— ì²˜ë¦¬í•  ìµœëŒ€ ê°œìˆ˜
    `
    
    const result = await this.pool.query(query)
    
    return result.rows.map(row => ({
      id: row.id,
      title: row.title,
      summary: row.summary,
      tags: row.tags || [],
      content_text: row.content_text,
      token_count: estimateTokenCount(row.content_text)
    }))
  }

  private async processBatch(items: ItemForEmbedding[]): Promise<BatchResult> {
    const result: BatchResult = {
      processed: items.length,
      succeeded: 0,
      failed: 0,
      skipped: 0,
      errors: []
    }

    try {
      // 1. í…ìŠ¤íŠ¸ ì¤€ë¹„ ë° ê²€ì¦
      const validItems: ItemForEmbedding[] = []
      for (const item of items) {
        const text = prepareTextForEmbedding(item)
        const tokenCount = estimateTokenCount(text)
        
        if (tokenCount > this.config.maxTokens) {
          console.warn(`âš ï¸  í† í° ì´ˆê³¼: ${item.id} (${tokenCount} tokens)`)
          result.skipped++
          continue
        }
        
        if (!text.trim()) {
          console.warn(`âš ï¸  ë¹ˆ í…ìŠ¤íŠ¸: ${item.id}`)
          result.skipped++
          continue
        }
        
        validItems.push({ ...item, content_text: text })
      }

      if (validItems.length === 0) {
        return result
      }

      // 2. OpenAI API í˜¸ì¶œ
      const texts = validItems.map(item => item.content_text)
      const embeddingResponse = await this.createEmbeddings(texts)

      // 3. DB ì—…ë°ì´íŠ¸
      for (let i = 0; i < validItems.length; i++) {
        const item = validItems[i]
        const embedding = embeddingResponse.data[i]?.embedding

        if (embedding) {
          await this.updateItemEmbedding(item.id, embedding)
          result.succeeded++
        } else {
          result.failed++
          result.errors.push({
            id: item.id,
            error: 'ì„ë² ë”© ìƒì„± ì‹¤íŒ¨'
          })
        }
      }

    } catch (error) {
      console.error(`âŒ ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜:`, error)
      
      // ê°œë³„ ì•„ì´í…œ ì¬ì‹œë„
      for (const item of items) {
        try {
          await this.processSingleItem(item)
          result.succeeded++
        } catch (singleError) {
          result.failed++
          result.errors.push({
            id: item.id,
            error: singleError instanceof Error ? singleError.message : 'Unknown error'
          })
        }
      }
    }

    return result
  }

  private async createEmbeddings(texts: string[]): Promise<any> {
    let lastError: Error | null = null

    for (let attempt = 1; attempt <= this.config.maxRetries; attempt++) {
      try {
        const response = await this.openai.embeddings.create({
          model: this.config.model,
          input: texts,
          encoding_format: 'float'
        })

        return response
      } catch (error) {
        lastError = error instanceof Error ? error : new Error('Unknown error')
        console.warn(`âš ï¸  ì„ë² ë”© ìƒì„± ì‹¤íŒ¨ (ì‹œë„ ${attempt}/${this.config.maxRetries}):`, lastError.message)
        
        if (attempt < this.config.maxRetries) {
          await delay(1000 * attempt) // ì§€ìˆ˜ ë°±ì˜¤í”„
        }
      }
    }

    throw lastError
  }

  private async processSingleItem(item: ItemForEmbedding): Promise<void> {
    const text = prepareTextForEmbedding(item)
    const response = await this.createEmbeddings([text])
    const embedding = response.data[0]?.embedding

    if (!embedding) {
      throw new Error('ì„ë² ë”© ìƒì„± ì‹¤íŒ¨')
    }

    await this.updateItemEmbedding(item.id, embedding)
  }

  private async updateItemEmbedding(id: string, embedding: number[]): Promise<void> {
    const query = `
      UPDATE ai_items 
      SET 
        embedding = $2::vector,
        embedding_model = $3,
        embedding_updated_at = NOW()
      WHERE id = $1
    `
    
    await this.pool.query(query, [
      id,
      JSON.stringify(embedding),
      this.config.model
    ])
  }

  async getProgressStats(): Promise<any> {
    const query = `
      SELECT 
        COUNT(*) as total_items,
        COUNT(embedding) as embedded_items,
        COUNT(*) - COUNT(embedding) as pending_items,
        ROUND(COUNT(embedding) * 100.0 / COUNT(*), 2) as progress_percent
      FROM ai_items
    `
    
    const result = await this.pool.query(query)
    return result.rows[0]
  }
}

// =====================================================
// CLI ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
// =====================================================

async function main() {
  const processor = new EmbeddingBatchProcessor(pool, openai, redis, config)

  try {
    // í˜„ì¬ ì§„í–‰ë¥  í™•ì¸
    const stats = await processor.getProgressStats()
    console.log('ğŸ“Š í˜„ì¬ ì„ë² ë”© ì§„í–‰ë¥ :', stats)

    // ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰
    const result = await processor.processAllPendingItems()
    
    // ê²°ê³¼ ì¶œë ¥
    console.log('\nğŸ“‹ ì²˜ë¦¬ ê²°ê³¼:')
    console.log(`   ì²˜ë¦¬ë¨: ${result.processed}ê°œ`)
    console.log(`   ì„±ê³µ: ${result.succeeded}ê°œ`)
    console.log(`   ì‹¤íŒ¨: ${result.failed}ê°œ`)
    console.log(`   ê±´ë„ˆëœ€: ${result.skipped}ê°œ`)
    
    if (result.errors.length > 0) {
      console.log('\nâŒ ì˜¤ë¥˜ ëª©ë¡:')
      result.errors.forEach(error => {
        console.log(`   ${error.id}: ${error.error}`)
      })
    }

    // ì—…ë°ì´íŠ¸ëœ ì§„í–‰ë¥  í™•ì¸
    const finalStats = await processor.getProgressStats()
    console.log('\nğŸ“Š ìµœì¢… ì„ë² ë”© ì§„í–‰ë¥ :', finalStats)

  } catch (error) {
    console.error('ğŸ’¥ ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨:', error)
    process.exit(1)
  } finally {
    await pool.end()
    await redis.quit()
  }
}

// CLIì—ì„œ ì‹¤í–‰ëœ ê²½ìš°
if (require.main === module) {
  main()
}

export { EmbeddingBatchProcessor, config }
```

## ë°°ì¹˜ ì‘ì—… ìŠ¤ì¼€ì¤„ë§

### Cron Job ì„¤ì •

```bash
# crontab -e
# ë§¤ì¼ ìƒˆë²½ 2ì‹œì— ì„ë² ë”© ë°°ì¹˜ ì²˜ë¦¬
0 2 * * * cd /path/to/ai-tools-website && npm run embedding:batch >> /var/log/embedding-batch.log 2>&1

# ë§¤ì‹œê°„ ì¦ë¶„ ì²˜ë¦¬ (ì‹ ê·œ/ì—…ë°ì´íŠ¸ëœ ì•„ì´í…œë§Œ)
0 * * * * cd /path/to/ai-tools-website && npm run embedding:incremental >> /var/log/embedding-incremental.log 2>&1
```

### package.json ìŠ¤í¬ë¦½íŠ¸

```json
{
  "scripts": {
    "embedding:batch": "ts-node scripts/embedding-batch.ts",
    "embedding:incremental": "ts-node scripts/embedding-batch.ts --incremental",
    "embedding:status": "ts-node scripts/embedding-status.ts",
    "embedding:reset": "ts-node scripts/embedding-reset.ts"
  }
}
```

## ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼

### ë°°ì¹˜ ì‘ì—… ëª¨ë‹ˆí„°ë§

```typescript
// scripts/embedding-monitor.ts
import { Pool } from 'pg'

interface MonitoringMetrics {
  total_items: number
  embedded_items: number
  pending_items: number
  progress_percent: number
  last_update: string
  avg_processing_time: number
  error_rate: number
}

async function getMonitoringMetrics(): Promise<MonitoringMetrics> {
  const pool = new Pool(/* config */)
  
  const query = `
    SELECT 
      COUNT(*) as total_items,
      COUNT(embedding) as embedded_items,
      COUNT(*) - COUNT(embedding) as pending_items,
      ROUND(COUNT(embedding) * 100.0 / COUNT(*), 2) as progress_percent,
      MAX(embedding_updated_at) as last_update
    FROM ai_items
  `
  
  const result = await pool.query(query)
  const row = result.rows[0]
  
  return {
    total_items: parseInt(row.total_items),
    embedded_items: parseInt(row.embedded_items),
    pending_items: parseInt(row.pending_items),
    progress_percent: parseFloat(row.progress_percent),
    last_update: row.last_update,
    avg_processing_time: 0, // ë³„ë„ ë¡œê·¸ì—ì„œ ê³„ì‚°
    error_rate: 0 // ë³„ë„ ë¡œê·¸ì—ì„œ ê³„ì‚°
  }
}

// Slack/Discord ì•Œë¦¼
async function sendAlert(message: string) {
  const webhookUrl = process.env.SLACK_WEBHOOK_URL
  if (!webhookUrl) return
  
  await fetch(webhookUrl, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      text: `ğŸ¤– ì„ë² ë”© ë°°ì¹˜ ì•Œë¦¼: ${message}`
    })
  })
}
```

### Prometheus ë©”íŠ¸ë¦­ ë…¸ì¶œ

```typescript
// scripts/metrics-server.ts
import express from 'express'
import { register, Gauge, Counter } from 'prom-client'

const app = express()

// ë©”íŠ¸ë¦­ ì •ì˜
const embeddingProgress = new Gauge({
  name: 'embedding_progress_percent',
  help: 'ì„ë² ë”© ì§„í–‰ë¥  (%)'
})

const embeddingTotal = new Gauge({
  name: 'embedding_total_items',
  help: 'ì „ì²´ ì•„ì´í…œ ìˆ˜'
})

const embeddingPending = new Gauge({
  name: 'embedding_pending_items',
  help: 'ëŒ€ê¸° ì¤‘ì¸ ì•„ì´í…œ ìˆ˜'
})

const embeddingErrors = new Counter({
  name: 'embedding_errors_total',
  help: 'ì„ë² ë”© ì˜¤ë¥˜ ì´ ê°œìˆ˜'
})

app.get('/metrics', async (req, res) => {
  try {
    const metrics = await getMonitoringMetrics()
    
    embeddingProgress.set(metrics.progress_percent)
    embeddingTotal.set(metrics.total_items)
    embeddingPending.set(metrics.pending_items)
    
    res.set('Content-Type', register.contentType)
    res.end(await register.metrics())
  } catch (error) {
    res.status(500).send('ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹¤íŒ¨')
  }
})

app.listen(9090)
```

## ë¹„ìš© ìµœì í™”

### OpenAI API ë¹„ìš© ê³„ì‚°

```typescript
interface CostEstimate {
  total_tokens: number
  estimated_cost_usd: number
  estimated_cost_krw: number
}

function calculateEmbeddingCost(tokenCount: number): CostEstimate {
  // OpenAI text-embedding-ada-002 ê°€ê²©: $0.0004 per 1K tokens
  const pricePerThousandTokens = 0.0004
  const exchangeRate = 1300 // ì›/ë‹¬ëŸ¬ (ì¶”ì •)
  
  const costUsd = (tokenCount / 1000) * pricePerThousandTokens
  const costKrw = costUsd * exchangeRate
  
  return {
    total_tokens: tokenCount,
    estimated_cost_usd: costUsd,
    estimated_cost_krw: costKrw
  }
}
```

### ì¤‘ë³µ ë°©ì§€ ì „ëµ

```sql
-- ë™ì¼í•œ í…ìŠ¤íŠ¸ì˜ ì¤‘ë³µ ì„ë² ë”© ë°©ì§€
CREATE INDEX ai_items_content_hash 
ON ai_items (md5(title || ' ' || COALESCE(summary, '') || ' ' || array_to_string(COALESCE(tags, '{}'), ' ')));

-- ì¤‘ë³µ ê²€ì‚¬ ì¿¼ë¦¬
SELECT 
  content_hash,
  COUNT(*) as duplicate_count,
  array_agg(id) as item_ids
FROM (
  SELECT 
    id,
    md5(title || ' ' || COALESCE(summary, '') || ' ' || array_to_string(COALESCE(tags, '{}'), ' ')) as content_hash
  FROM ai_items
  WHERE embedding IS NULL
) duplicates
GROUP BY content_hash
HAVING COUNT(*) > 1;
```

## ì˜¤ë¥˜ ì²˜ë¦¬ ë° ë³µêµ¬

### ì¬ì‹œë„ ë¡œì§

```typescript
async function processWithRetry<T>(
  operation: () => Promise<T>,
  maxRetries: number = 3,
  baseDelay: number = 1000
): Promise<T> {
  let lastError: Error

  for (let attempt = 1; attempt <= maxRetries; attempt++) {
    try {
      return await operation()
    } catch (error) {
      lastError = error instanceof Error ? error : new Error('Unknown error')
      
      if (attempt === maxRetries) {
        throw lastError
      }
      
      // ì§€ìˆ˜ ë°±ì˜¤í”„ + ì§€í„°
      const delay = baseDelay * Math.pow(2, attempt - 1) + Math.random() * 1000
      await new Promise(resolve => setTimeout(resolve, delay))
    }
  }
  
  throw lastError!
}
```

### ë°°ì¹˜ ì‘ì—… ë³µêµ¬

```sql
-- ì‹¤íŒ¨í•œ ì„ë² ë”© ì‹ë³„ ë° ì¬ì„¤ì •
UPDATE ai_items 
SET 
  embedding = NULL,
  embedding_updated_at = NULL
WHERE 
  embedding_updated_at < NOW() - INTERVAL '1 day'
  AND embedding IS NULL;

-- ì˜¤ë˜ëœ ì„ë² ë”© ì¬ìƒì„± (ëª¨ë¸ ì—…ë°ì´íŠ¸ ì‹œ)
UPDATE ai_items 
SET 
  embedding = NULL,
  embedding_updated_at = NULL
WHERE 
  embedding_model != 'text-embedding-ada-002'
  OR embedding_updated_at < '2024-01-01';
```

## ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### ë°°ì¹˜ í¬ê¸°ë³„ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸

```bash
# ë‹¤ì–‘í•œ ë°°ì¹˜ í¬ê¸°ë¡œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
npm run embedding:benchmark -- --batch-size=50 --items=1000
npm run embedding:benchmark -- --batch-size=100 --items=1000
npm run embedding:benchmark -- --batch-size=200 --items=1000
```

### ì˜ˆìƒ ì²˜ë¦¬ ì‹œê°„

| ë°°ì¹˜ í¬ê¸° | API í˜¸ì¶œ/ë¶„ | ì•„ì´í…œ/ì‹œê°„ | 1000ê°œ ì²˜ë¦¬ ì‹œê°„ |
|---------|----------|-----------|-------------|
| 50ê°œ | 30 | 1,500 | 40ë¶„ |
| 100ê°œ | 30 | 3,000 | 20ë¶„ |
| 200ê°œ | 30 | 6,000 | 10ë¶„ |

### ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰

```typescript
// ë©”ëª¨ë¦¬ ë° CPU ëª¨ë‹ˆí„°ë§
function getResourceUsage() {
  const usage = process.memoryUsage()
  const cpuUsage = process.cpuUsage()
  
  return {
    memory: {
      rss: Math.round(usage.rss / 1024 / 1024) + 'MB',
      heapTotal: Math.round(usage.heapTotal / 1024 / 1024) + 'MB',
      heapUsed: Math.round(usage.heapUsed / 1024 / 1024) + 'MB'
    },
    cpu: {
      user: cpuUsage.user,
      system: cpuUsage.system
    }
  }
}
```

---

> ë‹¤ìŒ ë‹¨ê³„: í”„ë¡ íŠ¸ì—”ë“œ ì—°ë™ ê°€ì´ë“œ ì‘ì„± (05-frontend-integration.md)