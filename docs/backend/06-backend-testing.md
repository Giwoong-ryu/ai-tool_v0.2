# Search V2 ë°±ì—”ë“œ í…ŒìŠ¤íŠ¸ ë° ë²¤ì¹˜ë§ˆí¬

Search V2 ë°±ì—”ë“œ ì‹œìŠ¤í…œì˜ ì¢…í•©ì ì¸ í…ŒìŠ¤íŠ¸ ë° ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ê°€ì´ë“œì…ë‹ˆë‹¤.

## í…ŒìŠ¤íŠ¸ ì „ëµ ê°œìš”

### í…ŒìŠ¤íŠ¸ í”¼ë¼ë¯¸ë“œ
1. **ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ (70%)**: ê°œë³„ í•¨ìˆ˜ ë° ëª¨ë“ˆ í…ŒìŠ¤íŠ¸
2. **í†µí•© í…ŒìŠ¤íŠ¸ (20%)**: API ì—”ë“œí¬ì¸íŠ¸ ë° ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™ í…ŒìŠ¤íŠ¸  
3. **E2E í…ŒìŠ¤íŠ¸ (10%)**: ì „ì²´ ê²€ìƒ‰ í”Œë¡œìš° í…ŒìŠ¤íŠ¸

### í…ŒìŠ¤íŠ¸ í™˜ê²½ êµ¬ì„±
- **ê°œë°œ í™˜ê²½**: ë¡œì»¬ PostgreSQL + Redis
- **CI í™˜ê²½**: Docker ì»¨í…Œì´ë„ˆ ê¸°ë°˜ í…ŒìŠ¤íŠ¸
- **ìŠ¤í…Œì´ì§• í™˜ê²½**: í”„ë¡œë•ì…˜ê³¼ ë™ì¼í•œ êµ¬ì„±

## ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

### í…ŒìŠ¤íŠ¸ ì„¤ì •

```typescript
// tests/setup.ts
import { Pool } from 'pg'
import { Redis } from 'ioredis'
import { config } from 'dotenv'

// í…ŒìŠ¤íŠ¸ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
config({ path: '.env.test' })

// í…ŒìŠ¤íŠ¸ ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
export const testPool = new Pool({
  host: process.env.TEST_DB_HOST || 'localhost',
  port: parseInt(process.env.TEST_DB_PORT || '5433'),
  database: process.env.TEST_DB_NAME || 'ai_tools_test',
  user: process.env.TEST_DB_USER || 'postgres',
  password: process.env.TEST_DB_PASSWORD || 'test123',
  max: 5
})

export const testRedis = new Redis({
  host: process.env.TEST_REDIS_HOST || 'localhost',
  port: parseInt(process.env.TEST_REDIS_PORT || '6380'),
  db: 1 // ë³„ë„ DB ì‚¬ìš©
})

// í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •ë¦¬
export async function cleanupTestData() {
  await testPool.query('TRUNCATE ai_items CASCADE')
  await testRedis.flushdb()
}

// ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ ë°ì´í„°
export const sampleItems = [
  {
    id: '550e8400-e29b-41d4-a716-446655440000',
    type: 'template',
    title: 'ë¸”ë¡œê·¸ ê¸€ ì´ˆì•ˆ ë§Œë“¤ê¸°',
    summary: 'SEO ìµœì í™”ëœ ë¸”ë¡œê·¸ ê¸€ êµ¬ì¡° ìƒì„±',
    tags: ['blog', 'writing', 'seo'],
    body: { category: 'writing' }
  },
  {
    id: '550e8400-e29b-41d4-a716-446655440001',
    type: 'tool',
    title: 'ChatGPT',
    summary: 'OpenAIì˜ ëŒ€í™”í˜• AI ì–´ì‹œìŠ¤í„´íŠ¸',
    tags: ['ai', 'chatbot', 'openai'],
    body: { category: 'ai-chat' }
  }
]
```

### ê²€ìƒ‰ ì„œë¹„ìŠ¤ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

```typescript
// tests/unit/searchService.test.ts
import { describe, test, expect, beforeEach, afterEach } from 'vitest'
import { SearchService } from '../../src/services/SearchService'
import { testPool, testRedis, cleanupTestData, sampleItems } from '../setup'

describe('SearchService', () => {
  let searchService: SearchService

  beforeEach(async () => {
    await cleanupTestData()
    searchService = new SearchService(testPool, testRedis)
    
    // í…ŒìŠ¤íŠ¸ ë°ì´í„° ì‚½ì…
    for (const item of sampleItems) {
      await testPool.query(
        'INSERT INTO ai_items (id, type, title, summary, tags, body) VALUES ($1, $2, $3, $4, $5, $6)',
        [item.id, item.type, item.title, item.summary, item.tags, JSON.stringify(item.body)]
      )
    }
  })

  afterEach(async () => {
    await cleanupTestData()
  })

  describe('search', () => {
    test('should return results for valid query', async () => {
      const result = await searchService.search({
        q: 'ë¸”ë¡œê·¸',
        page: 1,
        size: 10
      })

      expect(result.items).toHaveLength(1)
      expect(result.items[0].title).toContain('ë¸”ë¡œê·¸')
      expect(result.pagination.total).toBe(1)
      expect(result.performance.took_ms).toBeGreaterThan(0)
    })

    test('should return all items when no query provided', async () => {
      const result = await searchService.search({
        page: 1,
        size: 10
      })

      expect(result.items).toHaveLength(2)
      expect(result.pagination.total).toBe(2)
    })

    test('should filter by type', async () => {
      const result = await searchService.search({
        type: 'template',
        page: 1,
        size: 10
      })

      expect(result.items).toHaveLength(1)
      expect(result.items[0].type).toBe('template')
    })

    test('should handle pagination correctly', async () => {
      const page1 = await searchService.search({
        page: 1,
        size: 1
      })

      const page2 = await searchService.search({
        page: 2,
        size: 1
      })

      expect(page1.items).toHaveLength(1)
      expect(page2.items).toHaveLength(1)
      expect(page1.items[0].id).not.toBe(page2.items[0].id)
      expect(page1.pagination.pages).toBe(2)
    })

    test('should validate search parameters', async () => {
      await expect(searchService.search({
        q: 'a'.repeat(101), // ë„ˆë¬´ ê¸´ ì¿¼ë¦¬
        page: 1,
        size: 10
      })).rejects.toThrow('ê²€ìƒ‰ì–´ëŠ” 1-100ì ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤')

      await expect(searchService.search({
        page: 0, // ì˜ëª»ëœ í˜ì´ì§€
        size: 10
      })).rejects.toThrow()

      await expect(searchService.search({
        page: 1,
        size: 101 // ë„ˆë¬´ í° í˜ì´ì§€ í¬ê¸°
      })).rejects.toThrow()
    })
  })

  describe('getSuggestions', () => {
    test('should return suggestions for partial query', async () => {
      const result = await searchService.getSuggestions('ë¸”ë¡œ', 5)

      expect(result.suggestions).toHaveLength(1)
      expect(result.suggestions[0].title).toContain('ë¸”ë¡œê·¸')
      expect(result.suggestions[0].match_reason).toBe('title')
    })

    test('should limit suggestions count', async () => {
      const result = await searchService.getSuggestions('a', 1)

      expect(result.suggestions.length).toBeLessThanOrEqual(1)
    })

    test('should return empty for very short queries', async () => {
      const result = await searchService.getSuggestions('', 5)

      expect(result.suggestions).toHaveLength(0)
    })
  })

  describe('caching', () => {
    test('should cache search results', async () => {
      const params = { q: 'test', page: 1, size: 10 }
      
      // ì²« ë²ˆì§¸ ìš”ì²­
      const start1 = Date.now()
      const result1 = await searchService.search(params)
      const time1 = Date.now() - start1

      // ë‘ ë²ˆì§¸ ìš”ì²­ (ìºì‹œì—ì„œ)
      const start2 = Date.now()
      const result2 = await searchService.search(params)
      const time2 = Date.now() - start2

      expect(result1.items).toEqual(result2.items)
      expect(time2).toBeLessThan(time1 / 2) // ìºì‹œê°€ ë” ë¹¨ë¼ì•¼ í•¨
    })
  })
})
```

### ì„ë² ë”© ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸

```typescript
// tests/unit/embeddingBatch.test.ts
import { describe, test, expect, beforeEach, vi } from 'vitest'
import { EmbeddingBatchProcessor } from '../../scripts/embedding-batch'
import { testPool, testRedis, cleanupTestData } from '../setup'

// OpenAI API ëª¨í‚¹
const mockOpenAI = {
  embeddings: {
    create: vi.fn()
  }
}

describe('EmbeddingBatchProcessor', () => {
  let processor: EmbeddingBatchProcessor

  beforeEach(async () => {
    await cleanupTestData()
    processor = new EmbeddingBatchProcessor(testPool, mockOpenAI as any, testRedis, {
      batchSize: 2,
      maxRetries: 2,
      delayBetweenBatches: 100,
      model: 'text-embedding-ada-002',
      maxTokens: 8000
    })

    // í…ŒìŠ¤íŠ¸ ì•„ì´í…œ ì‚½ì… (ì„ë² ë”© ì—†ìŒ)
    await testPool.query(`
      INSERT INTO ai_items (type, title, summary, tags, body) VALUES 
      ('tool', 'Test Tool 1', 'Description 1', ARRAY['test'], '{}'),
      ('template', 'Test Template 1', 'Description 2', ARRAY['test'], '{}'),
      ('workflow', 'Test Workflow 1', 'Description 3', ARRAY['test'], '{}')
    `)
  })

  test('should process pending items in batches', async () => {
    // OpenAI API ëª¨í‚¹
    mockOpenAI.embeddings.create.mockResolvedValue({
      data: [
        { embedding: new Array(1536).fill(0.1) },
        { embedding: new Array(1536).fill(0.2) }
      ]
    })

    const result = await processor.processAllPendingItems()

    expect(result.succeeded).toBe(3)
    expect(result.failed).toBe(0)
    expect(mockOpenAI.embeddings.create).toHaveBeenCalledTimes(2) // 3ê°œ ì•„ì´í…œ, ë°°ì¹˜ í¬ê¸° 2

    // DBì— ì„ë² ë”©ì´ ì €ì¥ë˜ì—ˆëŠ”ì§€ í™•ì¸
    const embeddedItems = await testPool.query(
      'SELECT COUNT(*) as count FROM ai_items WHERE embedding IS NOT NULL'
    )
    expect(parseInt(embeddedItems.rows[0].count)).toBe(3)
  })

  test('should handle API failures with retry', async () => {
    mockOpenAI.embeddings.create
      .mockRejectedValueOnce(new Error('API Error'))
      .mockResolvedValue({
        data: [{ embedding: new Array(1536).fill(0.1) }]
      })

    const result = await processor.processAllPendingItems()

    expect(result.succeeded).toBeGreaterThan(0)
    expect(mockOpenAI.embeddings.create).toHaveBeenCalledTimes(4) // ì¬ì‹œë„ í¬í•¨
  })

  test('should skip items with too many tokens', async () => {
    // ë§¤ìš° ê¸´ í…ìŠ¤íŠ¸ë¥¼ ê°€ì§„ ì•„ì´í…œ ì¶”ê°€
    await testPool.query(`
      INSERT INTO ai_items (type, title, summary, tags, body) VALUES 
      ('tool', '${'very long text '.repeat(1000)}', 'Description', ARRAY['test'], '{}')
    `)

    const result = await processor.processAllPendingItems()

    expect(result.skipped).toBeGreaterThan(0)
  })

  test('should track progress correctly', async () => {
    const stats = await processor.getProgressStats()

    expect(stats.total_items).toBe(3)
    expect(stats.embedded_items).toBe(0)
    expect(stats.progress_percent).toBe(0)
  })
})
```

## í†µí•© í…ŒìŠ¤íŠ¸

### API ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸

```typescript
// tests/integration/searchAPI.test.ts
import { describe, test, expect, beforeAll, afterAll, beforeEach } from 'vitest'
import request from 'supertest'
import { app } from '../../src/app' // Express ì•±
import { testPool, testRedis, cleanupTestData, sampleItems } from '../setup'

describe('Search API Integration', () => {
  beforeAll(async () => {
    // í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì •
  })

  afterAll(async () => {
    await testPool.end()
    await testRedis.quit()
  })

  beforeEach(async () => {
    await cleanupTestData()
    
    // í…ŒìŠ¤íŠ¸ ë°ì´í„° ì‚½ì…
    for (const item of sampleItems) {
      await testPool.query(
        'INSERT INTO ai_items (id, type, title, summary, tags, body) VALUES ($1, $2, $3, $4, $5, $6)',
        [item.id, item.type, item.title, item.summary, item.tags, JSON.stringify(item.body)]
      )
    }
  })

  describe('GET /api/search', () => {
    test('should return search results', async () => {
      const response = await request(app)
        .get('/api/search')
        .query({ q: 'ë¸”ë¡œê·¸', page: 1, size: 10 })
        .expect(200)

      expect(response.body.items).toHaveLength(1)
      expect(response.body.items[0].title).toContain('ë¸”ë¡œê·¸')
      expect(response.body.pagination.total).toBe(1)
      expect(response.body.performance.took_ms).toBeGreaterThan(0)
    })

    test('should handle empty query', async () => {
      const response = await request(app)
        .get('/api/search')
        .query({ page: 1, size: 10 })
        .expect(200)

      expect(response.body.items).toHaveLength(2)
      expect(response.body.pagination.total).toBe(2)
    })

    test('should validate query parameters', async () => {
      await request(app)
        .get('/api/search')
        .query({ q: 'a'.repeat(101) })
        .expect(400)

      await request(app)
        .get('/api/search')
        .query({ page: 0 })
        .expect(400)

      await request(app)
        .get('/api/search')
        .query({ size: 101 })
        .expect(400)

      await request(app)
        .get('/api/search')
        .query({ type: 'invalid' })
        .expect(400)
    })

    test('should apply rate limiting', async () => {
      const requests = Array(10).fill(null).map(() =>
        request(app)
          .get('/api/search')
          .query({ q: 'test' })
      )

      const responses = await Promise.all(requests)
      
      // ëŒ€ë¶€ë¶„ì€ ì„±ê³µí•´ì•¼ í•˜ì§€ë§Œ, ì¼ë¶€ëŠ” rate limitì— ê±¸ë¦´ ìˆ˜ ìˆìŒ
      const successCount = responses.filter(r => r.status === 200).length
      expect(successCount).toBeGreaterThan(0)
    })

    test('should return proper error format', async () => {
      const response = await request(app)
        .get('/api/search')
        .query({ q: 'a'.repeat(101) })
        .expect(400)

      expect(response.body.error).toBeDefined()
      expect(response.body.error.code).toBe('INVALID_QUERY')
      expect(response.body.error.message).toContain('ê²€ìƒ‰ì–´ëŠ”')
    })
  })

  describe('GET /api/search/suggestions', () => {
    test('should return suggestions', async () => {
      const response = await request(app)
        .get('/api/search/suggestions')
        .query({ q: 'ë¸”ë¡œ', limit: 5 })
        .expect(200)

      expect(response.body.suggestions).toHaveLength(1)
      expect(response.body.suggestions[0].title).toContain('ë¸”ë¡œê·¸')
      expect(response.body.took_ms).toBeGreaterThan(0)
    })

    test('should limit suggestions count', async () => {
      const response = await request(app)
        .get('/api/search/suggestions')
        .query({ q: 'test', limit: 1 })
        .expect(200)

      expect(response.body.suggestions.length).toBeLessThanOrEqual(1)
    })

    test('should require query parameter', async () => {
      await request(app)
        .get('/api/search/suggestions')
        .expect(400)
    })
  })

  describe('HEAD /api/search', () => {
    test('should return health status', async () => {
      await request(app)
        .head('/api/search')
        .expect(200)
    })
  })

  describe('DELETE /api/search (dev only)', () => {
    test('should clear cache in development', async () => {
      process.env.NODE_ENV = 'development'
      
      await request(app)
        .delete('/api/search')
        .query({ pattern: 'search:*' })
        .expect(200)
    })

    test('should reject in production', async () => {
      process.env.NODE_ENV = 'production'
      
      await request(app)
        .delete('/api/search')
        .expect(403)
    })
  })
})
```

### ë°ì´í„°ë² ì´ìŠ¤ í†µí•© í…ŒìŠ¤íŠ¸

```typescript
// tests/integration/database.test.ts
import { describe, test, expect, beforeEach } from 'vitest'
import { testPool, cleanupTestData } from '../setup'

describe('Database Integration', () => {
  beforeEach(async () => {
    await cleanupTestData()
  })

  test('should create and query items with FTS', async () => {
    // ì•„ì´í…œ ì‚½ì…
    await testPool.query(`
      INSERT INTO ai_items (type, title, summary, tags, body) VALUES 
      ('tool', 'ChatGPT Tool', 'AI ëŒ€í™” ë„êµ¬', ARRAY['ai', 'chat'], '{}'),
      ('template', 'Blog Template', 'ë¸”ë¡œê·¸ í…œí”Œë¦¿', ARRAY['blog', 'writing'], '{}')
    `)

    // FTS ê²€ìƒ‰
    const result = await testPool.query(`
      SELECT title, ts_rank(tsv, plainto_tsquery('simple', 'ë¸”ë¡œê·¸')) as rank
      FROM ai_items
      WHERE tsv @@ plainto_tsquery('simple', 'ë¸”ë¡œê·¸')
      ORDER BY rank DESC
    `)

    expect(result.rows).toHaveLength(1)
    expect(result.rows[0].title).toBe('Blog Template')
    expect(result.rows[0].rank).toBeGreaterThan(0)
  })

  test('should handle vector operations when extension available', async () => {
    // pgvector í™•ì¥ í™•ì¸
    const extensionCheck = await testPool.query(`
      SELECT EXISTS(SELECT 1 FROM pg_extension WHERE extname = 'vector') as has_vector
    `)

    if (extensionCheck.rows[0].has_vector) {
      // ë²¡í„° ì„ë² ë”© í…ŒìŠ¤íŠ¸
      await testPool.query(`
        INSERT INTO ai_items (type, title, summary, tags, body, embedding) VALUES 
        ('tool', 'Test Tool', 'Test description', ARRAY['test'], '{}', $1::vector)
      `, [JSON.stringify(new Array(1536).fill(0.1))])

      const vectorResult = await testPool.query(`
        SELECT title, 1 - (embedding <-> $1::vector) as similarity
        FROM ai_items
        WHERE embedding IS NOT NULL
        ORDER BY similarity DESC
        LIMIT 1
      `, [JSON.stringify(new Array(1536).fill(0.1))])

      expect(vectorResult.rows).toHaveLength(1)
      expect(vectorResult.rows[0].similarity).toBeCloseTo(1.0, 1)
    }
  })

  test('should update generated tsvector column automatically', async () => {
    // ì•„ì´í…œ ì‚½ì…
    const insertResult = await testPool.query(`
      INSERT INTO ai_items (type, title, summary, tags, body) VALUES 
      ('tool', 'Test Tool', 'Test description', ARRAY['test', 'automation'], '{}')
      RETURNING id
    `)
    const itemId = insertResult.rows[0].id

    // tsvectorê°€ ìë™ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
    const tsvResult = await testPool.query(`
      SELECT tsv FROM ai_items WHERE id = $1
    `, [itemId])

    expect(tsvResult.rows[0].tsv).toBeTruthy()

    // ì œëª© ì—…ë°ì´íŠ¸ ì‹œ tsvectorë„ ì—…ë°ì´íŠ¸ë˜ëŠ”ì§€ í™•ì¸
    await testPool.query(`
      UPDATE ai_items SET title = 'Updated Tool' WHERE id = $1
    `, [itemId])

    const updatedTsvResult = await testPool.query(`
      SELECT tsv FROM ai_items WHERE id = $1
    `, [itemId])

    expect(updatedTsvResult.rows[0].tsv).not.toBe(tsvResult.rows[0].tsv)
  })

  test('should enforce constraints', async () => {
    // ì œëª© ê¸¸ì´ ì œì•½
    await expect(testPool.query(`
      INSERT INTO ai_items (type, title, summary, tags, body) VALUES 
      ('tool', '${'a'.repeat(201)}', 'Description', ARRAY['test'], '{}')
    `)).rejects.toThrow()

    // íƒ€ì… ì œì•½
    await expect(testPool.query(`
      INSERT INTO ai_items (type, title, summary, tags, body) VALUES 
      ('invalid', 'Test Tool', 'Description', ARRAY['test'], '{}')
    `)).rejects.toThrow()

    // íƒœê·¸ ê°œìˆ˜ ì œì•½
    await expect(testPool.query(`
      INSERT INTO ai_items (type, title, summary, tags, body) VALUES 
      ('tool', 'Test Tool', 'Description', $1, '{}')
    `, [new Array(21).fill('tag')])).rejects.toThrow()
  })
})
```

## ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### ë¶€í•˜ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

```typescript
// tests/performance/loadTest.ts
import { performance } from 'perf_hooks'
import { Pool } from 'pg'
import { Redis } from 'ioredis'
import { SearchService } from '../../src/services/SearchService'

interface BenchmarkResult {
  totalRequests: number
  successfulRequests: number
  failedRequests: number
  averageResponseTime: number
  p95ResponseTime: number
  p99ResponseTime: number
  requestsPerSecond: number
  errorRate: number
}

class SearchBenchmark {
  private searchService: SearchService
  private responseTimes: number[] = []

  constructor(pool: Pool, redis: Redis) {
    this.searchService = new SearchService(pool, redis)
  }

  async runBenchmark(options: {
    duration: number // ì´ˆ
    concurrency: number
    queries: string[]
  }): Promise<BenchmarkResult> {
    const { duration, concurrency, queries } = options
    const endTime = Date.now() + (duration * 1000)
    const promises: Promise<void>[] = []

    console.log(`ğŸš€ ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì‹œì‘: ${concurrency}ê°œ ë™ì‹œ ì—°ê²°, ${duration}ì´ˆ ë™ì•ˆ`)

    // ë™ì‹œ ìš”ì²­ ì‹¤í–‰
    for (let i = 0; i < concurrency; i++) {
      promises.push(this.runWorker(endTime, queries))
    }

    await Promise.all(promises)

    return this.calculateResults(duration)
  }

  private async runWorker(endTime: number, queries: string[]) {
    while (Date.now() < endTime) {
      const query = queries[Math.floor(Math.random() * queries.length)]
      
      try {
        const start = performance.now()
        await this.searchService.search({
          q: query,
          page: 1,
          size: 30
        })
        const responseTime = performance.now() - start
        this.responseTimes.push(responseTime)
      } catch (error) {
        this.responseTimes.push(-1) // ì‹¤íŒ¨ í‘œì‹œ
      }

      // ìš”ì²­ ê°„ ê°„ê²©
      await new Promise(resolve => setTimeout(resolve, Math.random() * 100))
    }
  }

  private calculateResults(duration: number): BenchmarkResult {
    const totalRequests = this.responseTimes.length
    const successfulRequests = this.responseTimes.filter(t => t > 0).length
    const failedRequests = totalRequests - successfulRequests
    const successfulTimes = this.responseTimes.filter(t => t > 0).sort((a, b) => a - b)

    const averageResponseTime = successfulTimes.reduce((sum, time) => sum + time, 0) / successfulTimes.length
    const p95ResponseTime = successfulTimes[Math.floor(successfulTimes.length * 0.95)] || 0
    const p99ResponseTime = successfulTimes[Math.floor(successfulTimes.length * 0.99)] || 0
    const requestsPerSecond = totalRequests / duration
    const errorRate = failedRequests / totalRequests

    return {
      totalRequests,
      successfulRequests,
      failedRequests,
      averageResponseTime,
      p95ResponseTime,
      p99ResponseTime,
      requestsPerSecond,
      errorRate
    }
  }
}

// ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
async function runPerformanceTest() {
  const pool = new Pool({
    host: 'localhost',
    port: 5432,
    database: 'ai_tools_bench',
    user: 'postgres',
    password: 'password',
    max: 20
  })

  const redis = new Redis({
    host: 'localhost',
    port: 6379,
    maxRetriesPerRequest: 3
  })

  const benchmark = new SearchBenchmark(pool, redis)

  const testQueries = [
    'ë¸”ë¡œê·¸',
    'ChatGPT',
    'AI ë„êµ¬',
    'í…œí”Œë¦¿',
    'ì›Œí¬í”Œë¡œìš°',
    'writing',
    'automation',
    'design'
  ]

  console.log('ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸')
  
  // ë‹¤ì–‘í•œ ë¶€í•˜ ìˆ˜ì¤€ í…ŒìŠ¤íŠ¸
  const loadLevels = [
    { concurrency: 10, duration: 30 },
    { concurrency: 50, duration: 30 },
    { concurrency: 100, duration: 30 },
    { concurrency: 200, duration: 30 }
  ]

  for (const level of loadLevels) {
    console.log(`\nğŸ”„ í…ŒìŠ¤íŠ¸: ${level.concurrency}ê°œ ë™ì‹œ ì—°ê²°`)
    
    const result = await benchmark.runBenchmark({
      ...level,
      queries: testQueries
    })

    console.log('ğŸ“‹ ê²°ê³¼:')
    console.log(`   ì´ ìš”ì²­: ${result.totalRequests}`)
    console.log(`   ì„±ê³µ: ${result.successfulRequests}`)
    console.log(`   ì‹¤íŒ¨: ${result.failedRequests}`)
    console.log(`   í‰ê·  ì‘ë‹µì‹œê°„: ${result.averageResponseTime.toFixed(2)}ms`)
    console.log(`   P95 ì‘ë‹µì‹œê°„: ${result.p95ResponseTime.toFixed(2)}ms`)
    console.log(`   P99 ì‘ë‹µì‹œê°„: ${result.p99ResponseTime.toFixed(2)}ms`)
    console.log(`   RPS: ${result.requestsPerSecond.toFixed(2)}`)
    console.log(`   ì˜¤ë¥˜ìœ¨: ${(result.errorRate * 100).toFixed(2)}%`)

    // ì„±ëŠ¥ ëª©í‘œ ê²€ì¦
    if (result.averageResponseTime > 300) {
      console.warn(`âš ï¸  í‰ê·  ì‘ë‹µì‹œê°„ì´ ëª©í‘œ(300ms)ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤: ${result.averageResponseTime.toFixed(2)}ms`)
    }

    if (result.errorRate > 0.01) {
      console.warn(`âš ï¸  ì˜¤ë¥˜ìœ¨ì´ ëª©í‘œ(1%)ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤: ${(result.errorRate * 100).toFixed(2)}%`)
    }

    // í…ŒìŠ¤íŠ¸ ê°„ ëŒ€ê¸°
    await new Promise(resolve => setTimeout(resolve, 5000))
  }

  await pool.end()
  await redis.quit()
}

if (require.main === module) {
  runPerformanceTest().catch(console.error)
}
```

### ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë¶„ì„

```sql
-- tests/performance/db-performance.sql

-- =====================================================
-- 1. ì¿¼ë¦¬ ì„±ëŠ¥ ë¶„ì„
-- =====================================================

-- ê²€ìƒ‰ ì¿¼ë¦¬ ì„±ëŠ¥ ë¶„ì„ (EXPLAIN ANALYZE)
EXPLAIN (ANALYZE, BUFFERS, FORMAT JSON)
WITH fts_candidates AS (
    SELECT 
        id, type, title, summary, tags, body, embedding,
        created_at, updated_at,
        ts_rank(tsv, plainto_tsquery('simple', 'blog writing')) as fts_rank
    FROM ai_items
    WHERE 
        tsv @@ plainto_tsquery('simple', 'blog writing')
        AND embedding IS NOT NULL
    ORDER BY fts_rank DESC
    LIMIT 200
),
vector_ranked AS (
    SELECT 
        *,
        1 - (embedding <-> '[ë”ë¯¸ ë²¡í„°]'::vector) as vector_similarity,
        (fts_rank * 0.3) + ((1 - (embedding <-> '[ë”ë¯¸ ë²¡í„°]'::vector)) * 0.7) as combined_score
    FROM fts_candidates
    ORDER BY combined_score DESC
)
SELECT 
    id, type, title, summary, tags, body,
    created_at, updated_at,
    fts_rank, vector_similarity, combined_score
FROM vector_ranked
LIMIT 30 OFFSET 0;

-- =====================================================
-- 2. ì¸ë±ìŠ¤ ì‚¬ìš©ë¥  ëª¨ë‹ˆí„°ë§
-- =====================================================

-- ì¸ë±ìŠ¤ ì‚¬ìš© í†µê³„
SELECT 
    schemaname,
    tablename,
    indexname,
    idx_scan as scans,
    idx_tup_read as tuples_read,
    idx_tup_fetch as tuples_fetched,
    CASE 
        WHEN idx_scan = 0 THEN 'UNUSED'
        WHEN idx_scan < 100 THEN 'LOW_USAGE'
        WHEN idx_scan < 1000 THEN 'MEDIUM_USAGE'
        ELSE 'HIGH_USAGE'
    END as usage_level
FROM pg_stat_user_indexes 
WHERE tablename = 'ai_items'
ORDER BY idx_scan DESC;

-- ì¸ë±ìŠ¤ í¬ê¸° ë° íš¨ìœ¨ì„±
SELECT 
    indexname,
    pg_size_pretty(pg_relation_size(indexname::regclass)) as index_size,
    idx_scan,
    CASE 
        WHEN idx_scan > 0 THEN pg_relation_size(indexname::regclass) / idx_scan
        ELSE pg_relation_size(indexname::regclass)
    END as bytes_per_scan
FROM pg_stat_user_indexes 
WHERE tablename = 'ai_items'
ORDER BY pg_relation_size(indexname::regclass) DESC;

-- =====================================================
-- 3. í…Œì´ë¸” í†µê³„ ë° ì„±ëŠ¥ ë©”íŠ¸ë¦­
-- =====================================================

-- í…Œì´ë¸” í¬ê¸° ë° ì„±ì¥ë¥ 
SELECT 
    'ai_items' as table_name,
    pg_size_pretty(pg_total_relation_size('ai_items')) as total_size,
    pg_size_pretty(pg_relation_size('ai_items')) as table_size,
    pg_size_pretty(pg_total_relation_size('ai_items') - pg_relation_size('ai_items')) as index_size,
    (SELECT COUNT(*) FROM ai_items) as row_count,
    pg_size_pretty(pg_total_relation_size('ai_items') / GREATEST((SELECT COUNT(*) FROM ai_items), 1)) as avg_row_size;

-- ì¿¼ë¦¬ ì„±ëŠ¥ í†µê³„ (PostgreSQL 13+ì—ì„œ pg_stat_statements í•„ìš”)
/*
SELECT 
    query,
    calls,
    total_exec_time,
    mean_exec_time,
    stddev_exec_time,
    rows,
    100.0 * shared_blks_hit / nullif(shared_blks_hit + shared_blks_read, 0) AS hit_percent
FROM pg_stat_statements 
WHERE query LIKE '%ai_items%'
ORDER BY mean_exec_time DESC
LIMIT 10;
*/

-- =====================================================
-- 4. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì¿¼ë¦¬
-- =====================================================

-- FTS ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ (ì—¬ëŸ¬ í‚¤ì›Œë“œ)
DO $$
DECLARE
    test_queries text[] := ARRAY['blog', 'writing', 'ai', 'chatgpt', 'template', 'automation'];
    query_text text;
    start_time timestamptz;
    end_time timestamptz;
    duration_ms numeric;
BEGIN
    FOREACH query_text IN ARRAY test_queries
    LOOP
        start_time := clock_timestamp();
        
        PERFORM COUNT(*)
        FROM ai_items
        WHERE tsv @@ plainto_tsquery('simple', query_text);
        
        end_time := clock_timestamp();
        duration_ms := EXTRACT(MILLISECONDS FROM (end_time - start_time));
        
        RAISE NOTICE 'Query "%" took %.2f ms', query_text, duration_ms;
    END LOOP;
END $$;

-- Vector ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ (ì„ë² ë”©ì´ ìˆëŠ” ê²½ìš°)
DO $$
DECLARE
    dummy_vector vector(1536) := (SELECT ARRAY(SELECT random() FROM generate_series(1, 1536)))::vector;
    start_time timestamptz;
    end_time timestamptz;
    duration_ms numeric;
    result_count integer;
BEGIN
    start_time := clock_timestamp();
    
    SELECT COUNT(*) INTO result_count
    FROM ai_items
    WHERE embedding IS NOT NULL
    ORDER BY embedding <-> dummy_vector
    LIMIT 100;
    
    end_time := clock_timestamp();
    duration_ms := EXTRACT(MILLISECONDS FROM (end_time - start_time));
    
    RAISE NOTICE 'Vector search for % items took %.2f ms', result_count, duration_ms;
END $$;

-- =====================================================
-- 5. ì„±ëŠ¥ ìµœì í™” ê¶Œì¥ì‚¬í•­
-- =====================================================

-- ë¯¸ì‚¬ìš© ì¸ë±ìŠ¤ ì‹ë³„
SELECT 
    schemaname,
    tablename,
    indexname,
    pg_size_pretty(pg_relation_size(indexname::regclass)) as size
FROM pg_stat_user_indexes 
WHERE idx_scan = 0 
  AND schemaname = 'public'
  AND pg_relation_size(indexname::regclass) > 1024 * 1024; -- 1MB ì´ìƒ

-- ì¤‘ë³µ ì¸ë±ìŠ¤ ê²€ì‚¬
WITH index_stats AS (
    SELECT 
        schemaname,
        tablename,
        indexname,
        array_to_string(array_agg(attname ORDER BY attnum), ',') as columns
    FROM pg_stats_ext pse
    JOIN pg_attribute pa ON pse.schemaname = pa.attrelid::regclass::text
    GROUP BY schemaname, tablename, indexname
)
SELECT 
    i1.tablename,
    i1.indexname as index1,
    i2.indexname as index2,
    i1.columns
FROM index_stats i1
JOIN index_stats i2 ON i1.tablename = i2.tablename 
    AND i1.columns = i2.columns 
    AND i1.indexname < i2.indexname
WHERE i1.tablename = 'ai_items';
```

### ëª¨ë‹ˆí„°ë§ ë° ì•ŒëŒ

```typescript
// tests/performance/monitoring.ts
import { Pool } from 'pg'
import { Redis } from 'ioredis'

interface PerformanceMetrics {
  timestamp: Date
  responseTime: {
    avg: number
    p95: number
    p99: number
  }
  throughput: {
    requestsPerSecond: number
    queriesPerSecond: number
  }
  database: {
    connectionCount: number
    slowQueries: number
    indexHitRatio: number
  }
  cache: {
    hitRatio: number
    memoryUsage: number
  }
  errors: {
    rate: number
    count: number
  }
}

class PerformanceMonitor {
  private pool: Pool
  private redis: Redis
  private metrics: PerformanceMetrics[] = []

  constructor(pool: Pool, redis: Redis) {
    this.pool = pool
    this.redis = redis
  }

  async collectMetrics(): Promise<PerformanceMetrics> {
    const timestamp = new Date()

    // ë°ì´í„°ë² ì´ìŠ¤ ë©”íŠ¸ë¦­
    const dbStats = await this.pool.query(`
      SELECT 
        sum(numbackends) as connection_count,
        sum(xact_commit) as commits,
        sum(xact_rollback) as rollbacks,
        sum(blks_hit) / (sum(blks_hit) + sum(blks_read)) as hit_ratio
      FROM pg_stat_database
      WHERE datname = current_database()
    `)

    // Redis ë©”íŠ¸ë¦­
    const redisInfo = await this.redis.info('memory')
    const memoryUsage = parseInt(redisInfo.match(/used_memory:(\d+)/)?.[1] || '0')
    
    const cacheStats = await this.redis.info('stats')
    const cacheHits = parseInt(cacheStats.match(/keyspace_hits:(\d+)/)?.[1] || '0')
    const cacheMisses = parseInt(cacheStats.match(/keyspace_misses:(\d+)/)?.[1] || '0')
    const hitRatio = cacheHits / (cacheHits + cacheMisses) || 0

    // ì„±ëŠ¥ ë©”íŠ¸ë¦­ êµ¬ì„±
    const metrics: PerformanceMetrics = {
      timestamp,
      responseTime: {
        avg: 0, // ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì• í”Œë¦¬ì¼€ì´ì…˜ ë©”íŠ¸ë¦­ì—ì„œ ìˆ˜ì§‘
        p95: 0,
        p99: 0
      },
      throughput: {
        requestsPerSecond: 0,
        queriesPerSecond: 0
      },
      database: {
        connectionCount: parseInt(dbStats.rows[0]?.connection_count || '0'),
        slowQueries: 0, // pg_stat_statementsì—ì„œ ìˆ˜ì§‘
        indexHitRatio: parseFloat(dbStats.rows[0]?.hit_ratio || '0')
      },
      cache: {
        hitRatio,
        memoryUsage
      },
      errors: {
        rate: 0,
        count: 0
      }
    }

    this.metrics.push(metrics)
    
    // ìµœê·¼ 24ì‹œê°„ ë°ì´í„°ë§Œ ìœ ì§€
    const oneDayAgo = new Date(Date.now() - 24 * 60 * 60 * 1000)
    this.metrics = this.metrics.filter(m => m.timestamp > oneDayAgo)

    return metrics
  }

  async checkAlerts(): Promise<string[]> {
    const alerts: string[] = []
    const latest = this.metrics[this.metrics.length - 1]

    if (!latest) return alerts

    // ì‘ë‹µ ì‹œê°„ ì•ŒëŒ
    if (latest.responseTime.avg > 500) {
      alerts.push(`ë†’ì€ í‰ê·  ì‘ë‹µì‹œê°„: ${latest.responseTime.avg}ms`)
    }

    // ë°ì´í„°ë² ì´ìŠ¤ ì•ŒëŒ
    if (latest.database.indexHitRatio < 0.95) {
      alerts.push(`ë‚®ì€ ì¸ë±ìŠ¤ íˆíŠ¸ìœ¨: ${(latest.database.indexHitRatio * 100).toFixed(2)}%`)
    }

    if (latest.database.connectionCount > 80) {
      alerts.push(`ë†’ì€ DB ì—°ê²° ìˆ˜: ${latest.database.connectionCount}`)
    }

    // ìºì‹œ ì•ŒëŒ
    if (latest.cache.hitRatio < 0.7) {
      alerts.push(`ë‚®ì€ ìºì‹œ íˆíŠ¸ìœ¨: ${(latest.cache.hitRatio * 100).toFixed(2)}%`)
    }

    // ì˜¤ë¥˜ìœ¨ ì•ŒëŒ
    if (latest.errors.rate > 0.01) {
      alerts.push(`ë†’ì€ ì˜¤ë¥˜ìœ¨: ${(latest.errors.rate * 100).toFixed(2)}%`)
    }

    return alerts
  }

  getMetrics(hours: number = 1): PerformanceMetrics[] {
    const cutoff = new Date(Date.now() - hours * 60 * 60 * 1000)
    return this.metrics.filter(m => m.timestamp > cutoff)
  }
}

export { PerformanceMonitor, type PerformanceMetrics }
```

## CI/CD í†µí•©

### GitHub Actions ì›Œí¬í”Œë¡œìš°

```yaml
# .github/workflows/backend-tests.yml
name: Backend Tests & Performance

on:
  push:
    branches: [main, develop]
    paths: ['src/services/**', 'scripts/**', 'tests/**']
  pull_request:
    branches: [main]

jobs:
  unit-tests:
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: pgvector/pgvector:pg15
        env:
          POSTGRES_DB: ai_tools_test
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: test123
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5433:5432
      
      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6380:6379

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Setup test database
        run: |
          npm run db:migrate:test
          npm run db:seed:test

      - name: Run unit tests
        run: npm run test:unit
        env:
          TEST_DB_HOST: localhost
          TEST_DB_PORT: 5433
          TEST_DB_NAME: ai_tools_test
          TEST_DB_USER: postgres
          TEST_DB_PASSWORD: test123
          TEST_REDIS_HOST: localhost
          TEST_REDIS_PORT: 6380

      - name: Run integration tests
        run: npm run test:integration

      - name: Upload coverage
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage/lcov.info

  performance-tests:
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    services:
      postgres:
        image: pgvector/pgvector:pg15
        env:
          POSTGRES_DB: ai_tools_bench
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: bench123
        ports:
          - 5432:5432
      
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Setup benchmark database
        run: |
          npm run db:setup:bench
          npm run db:seed:bench

      - name: Run performance tests
        run: npm run test:performance
        env:
          DB_HOST: localhost
          DB_PORT: 5432
          DB_NAME: ai_tools_bench
          DB_USER: postgres
          DB_PASSWORD: bench123
          REDIS_HOST: localhost
          REDIS_PORT: 6379

      - name: Upload performance results
        uses: actions/upload-artifact@v3
        with:
          name: performance-results
          path: test-results/performance/
```

## í…ŒìŠ¤íŠ¸ ë°ì´í„° ê´€ë¦¬

### í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ìŠ¤í¬ë¦½íŠ¸

```sql
-- scripts/generate-test-data.sql
-- ëŒ€ëŸ‰ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± (ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ìš©)

INSERT INTO ai_items (type, title, summary, tags, body)
SELECT 
    (ARRAY['tool', 'template', 'workflow'])[1 + floor(random() * 3)::int] as type,
    CASE 
        WHEN i % 3 = 0 THEN 'AI Tool ' || i
        WHEN i % 3 = 1 THEN 'Template ' || i || ' for Productivity'
        ELSE 'Workflow ' || i || ' Automation'
    END as title,
    CASE 
        WHEN i % 4 = 0 THEN 'Advanced AI-powered solution for productivity and automation'
        WHEN i % 4 = 1 THEN 'Template for creating professional content and documents'
        WHEN i % 4 = 2 THEN 'Workflow automation for streamlined business processes'
        ELSE 'Comprehensive tool for data analysis and visualization'
    END as summary,
    CASE 
        WHEN i % 5 = 0 THEN ARRAY['ai', 'productivity', 'automation']
        WHEN i % 5 = 1 THEN ARRAY['template', 'document', 'writing']
        WHEN i % 5 = 2 THEN ARRAY['workflow', 'business', 'process']
        WHEN i % 5 = 3 THEN ARRAY['data', 'analysis', 'visualization']
        ELSE ARRAY['tool', 'utility', 'helper']
    END as tags,
    json_build_object(
        'category', CASE WHEN i % 3 = 0 THEN 'ai-tools' WHEN i % 3 = 1 THEN 'templates' ELSE 'workflows' END,
        'features', ARRAY['feature1', 'feature2', 'feature3'],
        'difficulty', CASE WHEN i % 3 = 0 THEN 'beginner' WHEN i % 3 = 1 THEN 'intermediate' ELSE 'advanced' END
    ) as body
FROM generate_series(1, 10000) i;

-- ì¸ë±ìŠ¤ í†µê³„ ì—…ë°ì´íŠ¸
ANALYZE ai_items;
```

---

## ìš”ì•½

ì´ ë°±ì—”ë“œ í…ŒìŠ¤íŠ¸ ë° ë²¤ì¹˜ë§ˆí¬ ê°€ì´ë“œëŠ” Search V2 ì‹œìŠ¤í…œì˜ ì•ˆì •ì„±ê³¼ ì„±ëŠ¥ì„ ë³´ì¥í•˜ê¸° ìœ„í•œ ì¢…í•©ì ì¸ í…ŒìŠ¤íŠ¸ ì „ëµì„ ì œê³µí•©ë‹ˆë‹¤:

### ì£¼ìš” êµ¬ì„± ìš”ì†Œ:
1. **ë‹¨ìœ„ í…ŒìŠ¤íŠ¸**: ê°œë³„ í•¨ìˆ˜ ë° ëª¨ë“ˆì˜ ì •í™•ì„± ê²€ì¦
2. **í†µí•© í…ŒìŠ¤íŠ¸**: API ì—”ë“œí¬ì¸íŠ¸ì™€ ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™ ê²€ì¦
3. **ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬**: ë¶€í•˜ í…ŒìŠ¤íŠ¸ ë° ì‘ë‹µ ì‹œê°„ ì¸¡ì •
4. **ëª¨ë‹ˆí„°ë§**: ì‹¤ì‹œê°„ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ì•ŒëŒ
5. **CI/CD í†µí•©**: ìë™í™”ëœ í…ŒìŠ¤íŠ¸ íŒŒì´í”„ë¼ì¸

### ì„±ëŠ¥ ëª©í‘œ:
- **ì‘ë‹µ ì‹œê°„**: P95 < 300ms, P99 < 500ms
- **ì²˜ë¦¬ëŸ‰**: 500 RPS ì´ìƒ
- **ì˜¤ë¥˜ìœ¨**: 1% ë¯¸ë§Œ
- **ê°€ìš©ì„±**: 99.9% ì´ìƒ

ì´ ê°€ì´ë“œë¥¼ í†µí•´ Search V2 ì‹œìŠ¤í…œì˜ í’ˆì§ˆê³¼ ì„±ëŠ¥ì„ ì§€ì†ì ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§í•˜ê³  ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.